{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **Unzip the file**"
      ],
      "metadata": {
        "id": "mJbXjKkKIS1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = 'easy_ham.zip'\n",
        "\n",
        "\n",
        "extracted_dir = 'data/'\n",
        "\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "for filename in os.listdir(extracted_dir):\n",
        "    file_path = os.path.join(extracted_dir, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "\n",
        "            content = file.read()"
      ],
      "metadata": {
        "id": "7L6gyHrcoGWu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cleaning data**"
      ],
      "metadata": {
        "id": "clQh1J6Hwsrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(r'[^\\w\\s]', '', text) #to remove ponctuation\n",
        "  text = re.sub(r'\\d+', '', text) #to remove spacing\n",
        "  text = text.lower()\n",
        "\n",
        "  return text\n",
        "\n",
        "def clean_files_in_directory(directory):\n",
        "    cleaned_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file_name in files:\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    cleaned_text = clean_text(text)\n",
        "                    cleaned_files.append((file_path, cleaned_text))\n",
        "            except UnicodeDecodeError:\n",
        "                # Try opening the file with a different encoding\n",
        "                with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                    text = file.read()\n",
        "                    cleaned_text = clean_text(text)\n",
        "                    cleaned_files.append((file_path, cleaned_text))\n",
        "    return cleaned_files\n",
        "\n",
        "\n",
        "root_directory = 'easy_ham'\n",
        "\n",
        "cleaned_files = clean_files_in_directory(root_directory)\n",
        "\n",
        "for file_path, cleaned_text in cleaned_files:\n",
        "    print(f\"File: {file_path}\")\n",
        "    print(f\"Cleaned text: {cleaned_text}\")\n",
        "    print(\"---------------------------------------------\")"
      ],
      "metadata": {
        "id": "iulcQHa1yIv-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "3WjfXxMP5uTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def tokenize_text(cleaned_text):\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "    tokens = [token.lower() for token in tokens] #to convert to lowercase\n",
        "    return tokens\n",
        "\n",
        "\n",
        "for file_path, cleaned_text in cleaned_files:\n",
        "\n",
        "    tokens = tokenize_text(cleaned_text)\n",
        "\n",
        "    print(f\"Tokens for {file_path}:\")\n",
        "    print(tokens)\n",
        "    print(\"---------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "-q3F6fts_GMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f69b3b7-aa8f-45a8-9c73-d1f260212d3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**remove stopwords from the text token**"
      ],
      "metadata": {
        "id": "gygCITTKCXa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "all_tokens_without_stopwords = []\n",
        "\n",
        "for file_path, cleaned_text in cleaned_files:\n",
        "\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "    tokens_without_stopwords = [token for token in tokens if token.lower() not in stop_words] #here is the tokens after removing stopwords\n",
        "\n",
        "    all_tokens_without_stopwords.append((file_path, tokens_without_stopwords))\n",
        "\n",
        "for file_path, tokens_without_stopwords in all_tokens_without_stopwords:\n",
        "    print(f\"File: {file_path}\")\n",
        "    print(\"Tokens without stopwords:\")\n",
        "    print(tokens_without_stopwords)\n",
        "    print(\"---------------------------------------------\")"
      ],
      "metadata": {
        "id": "Ig-GaqP6Eht4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4200739f-4d23-446c-fc2b-0d3d1062e7a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}