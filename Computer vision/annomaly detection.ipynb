{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description : Anomaly Detection is a binary classification identifying unusual or unexpected patterns in a dataset, which deviate significantly from the majority of the data. The goal of anomaly detection is to identify such anomalies, which could represent errors, fraud, or other types of unusual events, and flag them for further investigation, in the provided dataset, there are 12 industrial products, all labeled with normal or abnormal, the products which are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VisA dataset contains 12 subsets corresponding to 12 different objects as shown in the above figure. There are 10,821 images with 9,621 normal and 1,200 anomalous samples. Four subsets are different types of printed circuit boards (PCB) with relatively complex structures containing transistors, capacitors, chips, etc. For the case of multiple instances in a view, we collect four subsets: Capsules, Candles, Macaroni1 and Macaroni2. Instances in Capsules and Macaroni2 largely differ in locations and poses. Moreover, we collect four subsets including Cashew, Chewing gum, Fryum and Pipe fryum, where objects are roughly aligned. The anomalous images contain various flaws, including surface defects such as scratches, dents, color spots or crack, and structural defects like misplacement or missing parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The objective of this notebook is to build a deep learning model to detect anomalies in the VisA dataset. The model will be trained on the normal images and then tested on the normal and abnormal images. The model will be evaluated based on the F1 score.\n",
    "\n",
    "# Approach\n",
    "The approach is as follows:\n",
    "1. Load the data\n",
    "2. Preprocess the data\n",
    "3. Build the model\n",
    "4. Train the model\n",
    "5. Evaluate the model\n",
    "\n",
    "# References\n",
    "1. [VisA: A Dataset for Anomaly Detection in Industrial Visual Inspection](https://arxiv.org/abs/2103.04517)\n",
    "2. [Anomaly Detection in Industrial Visual Inspection: A Survey](https://arxiv.org/abs/2103.04517)\n",
    "3. [Anomaly Detection in Industrial Visual Inspection: A Survey](https://arxiv.org/abs/2103.04517)\n",
    "4. [Anomaly Detection in Industrial Visual Inspection: A Survey](https://arxiv.org/abs/2103.04517)\n",
    "5. [Anomaly Detection in Industrial Visual Inspection: A Survey](https://arxiv.org/abs/2103.04517)\n",
    "\n",
    "# Contents\n",
    "1. [Load the data](#1.-Load-the-data)\n",
    "2. [Preprocess the data](#2.-Preprocess-the-data)\n",
    "3. [Build the model](#3.-Build-the-model)\n",
    "4. [Train the model](#4.-Train-the-model)\n",
    "5. [Evaluate the model](#5.-Evaluate-the-model)\n",
    "6. [Make predictions](#6.-Make-predictions)\n",
    "7. [Conclusion](#7.-Conclusion)\n",
    "8. [References](#8.-References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m        VisA\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m        |-- candle\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m        |-----|----- ...\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# image_annot.csv gives image-level label and pixel-level annotation mask for each image. \u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#The id2class map functions for multi-class masks can be found in ./utils/id2class.py Here the masks for normal images are not stored to save space.\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Download the data\n",
    "    # The data is available at: https://amazon-visual-anomaly.s3.us-west-2.amazonaws.com/VisA_20220922.tar\n",
    "    # The data is in the form of a tar file. We will download the tar file and extract the contents.\n",
    "    # The data tree of the downloaded data is as follows.\n",
    "\"\"\"\n",
    "        VisA\n",
    "        |-- candle\n",
    "        |-----|--- Data\n",
    "        |-----|-----|----- Images\n",
    "        |-----|-----|--------|------ Anomaly \n",
    "        |-----|-----|--------|------ Normal \n",
    "        |-----|-----|----- Masks\n",
    "        |-----|-----|--------|------ Anomaly \n",
    "        |-----|--- image_anno.csv\n",
    "        |-- capsules\n",
    "        |-----|----- ...\n",
    "    \"\"\"\n",
    "    # image_annot.csv gives image-level label and pixel-level annotation mask for each image. \n",
    "    #The id2class map functions for multi-class masks can be found in ./utils/id2class.py Here the masks for normal images are not stored to save space.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the data\n",
    "!wget https://amazon-visual-anomaly.s3.us-west-2.amazonaws.com/VisA_20220922.tar\n",
    "\n",
    "# Define the data directory\n",
    "!mkdir VisA\n",
    "data_dir = 'VisA'\n",
    "\n",
    "# Extract the data into a file named VisA\n",
    "!tar -xvf VisA_20220922.tar -C VisA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Define the image directory\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdata_dir\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandle/Data/Images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define the image annotation file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m image_anno_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandle/image_anno.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the image directory\n",
    "image_dir = os.path.join(data_dir, 'candle/Data/Images')\n",
    "\n",
    "# Define the image annotation file\n",
    "image_anno_file = os.path.join(data_dir, 'candle/image_anno.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the image annotation file\n",
    "image_anno = pd.read_csv(image_anno_file)\n",
    "\n",
    "# Display the first few rows of the image annotation file\n",
    "image_anno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(image, size=(224, 224)):\n",
    "    # This function resizes the image to the given size.\n",
    "    image = cv2.resize(image, size)\n",
    "    return image\n",
    "\n",
    "def normalize_image(image):\n",
    "    \n",
    "    #This function normalizes the image.\n",
    "    \n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_anno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# for each image in the image directory we will resize the image to the size (256, 256) and normalize the image.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# We will store the resized and normalized image in the image list respectively.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m \u001b[43mimage_anno\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      7\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, image)\n\u001b[0;32m      8\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_anno' is not defined"
     ]
    }
   ],
   "source": [
    "# for each image in the image directory we will resize the image to the size (256, 256) and normalize the image.\n",
    "# We will store the resized and normalized image in the image list respectively.\n",
    "\n",
    "images = []\n",
    "\n",
    "for image in image_anno['image']:\n",
    "    image_path = os.path.join(image_dir, image)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = resize_image(image)\n",
    "    image = normalize_image(image)\n",
    "    images.append(image)\n",
    "\n",
    "# Convert the images list to a numpy array\n",
    "images = np.array(images)\n",
    "\n",
    "# Display the shape of the images array\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Handling\n",
    "# The image_anno file contains a lable column that indicates whether the image is normal or anomalous.\n",
    "# We will convert the labels into 0 and 1 where 0 indicates normal and 1 indicates anomalous.\n",
    "# the normal images are labeled \"noemal\" and the anomalous images are labeled with any other str.\n",
    "\n",
    "# Define the labels\n",
    "labels = np.where(image_anno['label'] == 'normal', 0, 1)\n",
    "\n",
    "# Display the labels\n",
    "labels\n",
    "\n",
    "# replace the label column in the image_anno dataframe with the new labels\n",
    "image_anno['label'] = labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "images_train, images_val = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "# split the labels into training and validation sets\n",
    "labels_train, labels_val = train_test_split(labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and validation sets\n",
    "print(\"Training set shape:\", images_train.shape)\n",
    "print(\"Validation set shape:\", images_val.shape)\n",
    "\n",
    "# Data Augmentation (Optional): We can use data augmentation to increase the diversity of the training data.\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Use the data generator to create batches of augmented images during training\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a data generator for the training set\n",
    "train_datagen = datagen.flow(images_train, batch_size=batch_size)\n",
    "\n",
    "# Create a data generator for the validation set\n",
    "val_datagen = datagen.flow(images_val, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained model\n",
    "# model : vgg16\n",
    "\n",
    "from tensorflow.keras.applications import VGG16  # Import the VGG16 model\n",
    "\n",
    "# Load the pre-trained model without the top layers (classification head)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers (optional, to prevent retraining pre-trained weights)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Use the base model for feature extraction\n",
    "def extract_features(img):\n",
    "    img = np.expand_dims(img, axis=0)  # Add a batch dimension\n",
    "    features = base_model.predict(img)  # Pass the image through the model\n",
    "    return features.flatten()  # Flatten the feature vector\n",
    "\n",
    "# Extract features from the images\n",
    "features_train = np.array([extract_features(img) for img in images_train])\n",
    "features_train = np.asarray(features_train).astype('float32')\n",
    "features_val = np.array([extract_features(img) for img in images_val])\n",
    "features_val = np.asarray(features_val).astype('float32')\n",
    "\n",
    "# Display the shape of the extracted features\n",
    "features_train.shape, features_val.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(features_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"feature train :\")\n",
    "print(features_train)\n",
    "print(\"feature val :\")\n",
    "print(features_val) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(features_train, np\u001b[38;5;241m.\u001b[39mones(features_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), validation_data\u001b[38;5;241m=\u001b[39m(features_val, np\u001b[38;5;241m.\u001b[39mones(features_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(features_val, np\u001b[38;5;241m.\u001b[39mones(features_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(features_train, labels_train, validation_data=(features_val, labels_val), epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(features_val, np.ones(features_val.shape[0]) )\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict the labels for the validation set\n",
    "preds = model.predict(features_val)\n",
    "\n",
    "# Convert the predictions to binary labels\n",
    "pred_labels = np.where(preds > 0.5, 1, 0)\n",
    "\n",
    "# Display the predicted labels\n",
    "pred_labels\n",
    "\n",
    "# Display the true labels\n",
    "labels_val\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels_val, pred_labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on a new image\n",
    "# Load a new image\n",
    "new_image_path = 'VisA/candle/Data/Images/Anomaly/Anomaly_0001.jpg'\n",
    "new_image = cv2.imread(new_image_path)\n",
    "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
    "new_image = resize_image(new_image)\n",
    "new_image = normalize_image(new_image)\n",
    "\n",
    "# Extract features from the new image\n",
    "new_features = extract_features(new_image)\n",
    "\n",
    "# Reshape the features\n",
    "new_features = new_features.reshape(1, -1)\n",
    "\n",
    "# Predict the label for the new image\n",
    "new_pred = model.predict(new_features)\n",
    "\n",
    "# Convert the prediction to a binary label\n",
    "new_pred_label = np.where(new_pred > 0.5, 1, 0)\n",
    "\n",
    "# Display the new image and the predicted label\n",
    "plt.imshow(new_image)\n",
    "plt.axis('off')\n",
    "plt.title('Predicted Label: {}'.format(new_pred_label[0]))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('anomaly_detection_model.h5')\n",
    "\n",
    "# Save the feature extraction model\n",
    "base_model.save('feature_extraction_model.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
